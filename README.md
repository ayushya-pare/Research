# Research

Development of new gradient descent optimization algorithms to improve training of deep neural networks

## 1. Guided Exploration
The Guided Exploration (GE) algorithm is designed to improve convergence in gradient descent optimization for machine learning models. By combining local and global search strategies, GE navigates complex loss landscapes, achieving better accuracy and convergence compared to traditional optimizers.


----------------------------
## 2. Inverse Parameter Displacement SGD Optimizer

A new adaptive Stochastic Gradient Descent (SGD) algorithm that dynamically adjusts the learning rate for each model parameter based on the displacement of the parameter values across iterations. The approach enhances convergence and model accuracy, outperforming traditional optimizers like SGD with momentum and Adam.

## 3. Envelope-LMS Multipath Suppression
An adaptive algorithm to suppress **Multipath Interference (MpI)** in **Time-of-Flight (ToF)** cameras by leveraging the amplitude envelope of the transmitted signal. The method uses an adaptive Least Mean Squares (LMS) filter to minimize deviations caused by MpI, improving the accuracy of depth measurements significantly..#